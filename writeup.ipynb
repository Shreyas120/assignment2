{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p align=\"center\" style=\"color:rgb(55, 113, 197);\">[16-825: Learning for 3D Vision](https://learning3d.github.io/)</p>\n",
    "### <p align=\"center\" style=\"color:rgb(0, 0, 0);\">Rendering Basics with PyTorch3D</p>\n",
    "<p align=\"center\" style=\"color:rgb(55, 113, 197);\">\n",
    "    Shreyas Jha &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    <a href=\"https://github.com/Shreyas120/assignment2\">Code</a> \n",
    "</p>\n",
    "\n",
    "<!-- Color theme\n",
    "    Dark color:rgb(42, 88, 153)\n",
    "    Medium (55, 113, 197)\n",
    "    Light and bright color:rgb(68, 129, 211)\n",
    " -->\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p align=\"center\" style=\"color:rgb(55, 113, 197);\">1.&nbsp;Exploring loss functions </p>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">1.1.&nbsp;Fitting a voxel grid </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![Fit pointel GIF](./data/shreyasj/1/fit_vox.gif)</p>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">1.2.&nbsp;Fitting a point cloud </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![Fit point clouds GIF](./data/shreyasj/1/fit_point.gif)</p>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">1.3.&nbsp;Fitting a mesh </p>\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![Fit Mesh GIF](./data/shreyasj/1/fit_mesh.gif)</p>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## <p align=\"center\" style=\"color:rgb(55, 113, 197);\">2.&nbsp;Reconstructing 3D from single view</p>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">2.1.&nbsp;Image to voxel grid</p>\n",
    "\n",
    "| <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Input image </p> | <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3D Structures </p> |\n",
    "| -------- | ---------------- | \n",
    "| ![I/p image](./data/shreyasj/2/eval_vox_gtimg_110.png) | ![GIF](./data/shreyasj/2/eval_vox_110.gif) | \n",
    "| ![I/p image](./data/shreyasj/2/eval_vox_gtimg_220.png) | ![GIF](./data/shreyasj/2/eval_vox_220.gif) | \n",
    "| ![I/p image](./data/shreyasj/2/eval_vox_gtimg_330.png) | ![GIF](./data/shreyasj/2/eval_vox_330.gif) | \n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">2.2.&nbsp;Image to point cloud</p>\n",
    "\n",
    "| <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Input image </p> | <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3D Structures </p> |\n",
    "| -------- | ---------------- | \n",
    "| ![I/p image](./data/shreyasj/2/eval_point_gtimg_110.png) | ![GIF](./data/shreyasj/2/eval_point_110.gif) | \n",
    "| ![I/p image](./data/shreyasj/2/eval_point_gtimg_220.png) | ![GIF](./data/shreyasj/2/eval_point_220.gif) | \n",
    "| ![I/p image](./data/shreyasj/2/eval_point_gtimg_330.png) | ![GIF](./data/shreyasj/2/eval_point_330.gif) | \n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">2.3.&nbsp;Image to mesh</p>\n",
    "\n",
    "| <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Input image </p> | <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3D Structures </p> |\n",
    "| -------- | ---------------- | \n",
    "| ![I/p image](./data/shreyasj/2/eval_mesh_gtimg_110.png) | ![GIF](./data/shreyasj/2/eval_mesh_110.gif) | \n",
    "| ![I/p image](./data/shreyasj/2/eval_mesh_gtimg_220.png) | ![GIF](./data/shreyasj/2/eval_mesh_220.gif) | \n",
    "| ![I/p image](./data/shreyasj/2/eval_mesh_gtimg_330.png) | ![GIF](./data/shreyasj/2/eval_mesh_330.gif) | \n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">2.4.&nbsp;Quantitative comparisions</p>\n",
    "\n",
    "| <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Image to voxel grid </p> | <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Image to point cloud </p> | <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Image to mesh </p> |\n",
    "| -------- | ---------------- |  ---------------- | \n",
    "| ![Eval image vox](./data/shreyasj/2/2.4/eval_vox.png) | ![Eval image point](./data/shreyasj/2/2.4/eval_point.png) | ![Eval image mesh](./data/shreyasj/2/2.4/eval_mesh.png)  |\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">2.5.&nbsp;Effects of hyperparameter variations</p>\n",
    "\n",
    "| <p align=\"center\" style=\"color:rgb(55, 113, 197);\">1k points </p> | <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3k points </p> | <p align=\"center\" style=\"color:rgb(55, 113, 197);\">7.5k points </p> |\n",
    "| -------- | ---------------- |  ---------------- | \n",
    "| ![Eval image point 1,000](./data/shreyasj/2/2.4/eval_point.png) | ![Eval image point 3,000](./data/shreyasj/2/2.4/eval_point_3.png) | ![Eval image point 7,500](./data/shreyasj/2/2.4/eval_point_75.png)  | \n",
    "\n",
    "#### <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![Loss curves](./data/shreyasj/2/loss_npoints_vary.png)</p>\n",
    "\n",
    "##### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">\n",
    "Analyzing the impact of the number of points in a point cloud on model convergence and performance reveals key insights into the training process of 3D reconstruction models. Initially, models with 1000, 3000, and 7500 points all start with similar loss values. However, as training progresses, models with a higher number of points exhibit a lower loss and smoother convergence, indicative of a more detailed and nuanced representation of the 3D shape, leading to a more accurate reconstruction as measured by the Chamfer distance. \n",
    "\n",
    "### Hyperparameter Variation Analysis\n",
    "\n",
    "More points provide a stronger and possibly clearer gradient signal during backpropagation, as there is more data to inform the loss calculation. This can help the optimizer make more accurate updates to the model parameters, leading to faster convergence. The Chamfer distance (used as the loss function) might have a smoother landscape with more points, reducing the number of local minima and making it easier for the optimizer to find a good path toward the global minimum.\n",
    "\n",
    "The **number of points** in the point cloud, chosen as the hyperparameter for this analysis, significantly affects the model's performance and convergence:\n",
    "\n",
    "<div style=\"text-align: left;\">\n",
    "\n",
    "- **Regularization Effect**: Increasing the number of points acts as an implicit regularizer, requiring the model to maintain consistency over a larger set of predictions, which can help prevent overfitting.\n",
    "- **Computational Considerations**: A larger number of points leads to better performance but requires increased computational resources. This necessitates a balance between accuracy and computational efficiency.\n",
    "- **Outlier Management**: More points mitigate the relative impact of outliers on the loss, though additional points beyond a certain threshold may not significantly improve performance and could introduce computational challenges or overfitting.\n",
    "\n",
    "</div>\n",
    "\n",
    "### Conclusive Insights\n",
    "\n",
    "We can conclude that the number of points in a point cloud is a critical hyperparameter influencing the model's accuracy in reconstructing 3D shapes and the stability of its training process. Determining the optimal number of points involves balancing the enhanced performance with the additional computational costs and the potential for overfitting. For applications sensitive to outliers, it may be beneficial to incorporate outlier removal post-processing or to adjust the model architecture or loss function for greater robustness.\n",
    "</p>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">2.6.&nbsp;Model Interpretation</p>\n",
    "\n",
    "The below interpretation is for the model to predict 3D voxels and the decorder is inspired by Pix2Vox. The process starts with a high-dimensional, low-resolution feature representation and gradually increases the resolution while reducing the dimensionality of the feature maps. The initial feature representation with shape [1, 256, 2, 2, 2]. This tensor is high-dimensional but spatially coarse, encoding the most abstract representation of the input data. With each successive layer, the blocks become smaller and more numerous, indicating an increase in the spatial resolution and a refinement of the feature representation. The colors in these layers likely signify different feature activations, with the network beginning to spatially organize these features in a more detailed manner. The final layer produces a binary voxel representation with a shape of [1, 1, 32, 32, 32]. This indicates that all the spatial and feature information has been condensed into a single binary channel, representing the 3D structure. \n",
    "\n",
    "Each layer is squeezed to remove the batch dimension, and RGB channels are derived from different statistical measures (maximum, mean, minimum) across the feature dimension. These channels are normalized and combined to colorize the voxels, with voxel presence determined by activations exceeding a threshold. The resulting visualizations are displayed in a series of subplots, providing a comparative view of the input and the transformations occurring within each layer.\n",
    "\n",
    "\n",
    "![Interpretation 1](./data/shreyasj/2/interpret_2.png)\n",
    "![Interpretation 1](./data/shreyasj/2/interpret_4.png)\n",
    "![Interpretation 1](./data/shreyasj/2/interpret_6.png)\n",
    "![Interpretation 1](./data/shreyasj/2/interpret_5.png)\n",
    "\n",
    "\n",
    "Initially, the visualizations show stark contrast in feature vectors with vibrant color differentiation, indicating diverse high-level feature detection; as layers progress, color variation diminishes, suggesting a shift toward spatial refinement. In the final layer, after applying a sigmoid function, the network's focus narrows to a binary volume representation, emphasizing the structural essence of the object over color features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3.&nbsp;Exploring other architectures / datasets</p>\n",
    "\n",
    "### <p align=\"center\" style=\"color:rgb(55, 113, 197);\">3.3.&nbsp;Extended dataset for training</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <p align=\"center\" style=\"color:rgb(55, 113, 197);\">Late days used</p>\n",
    "\n",
    "## <p align=\"center\" style=\"color:rgb(42, 88, 153);\">![Two](./data/shreyasj/late/two.png)</p>\n",
    "<p align=\"center\" style=\"color:rgb(55, 113, 197);\">\n",
    "    <a href=\"https://github.com/learning3d/learning3d.github.io/tree/main/spring23/data/late_days\">Image Source</a> \n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
